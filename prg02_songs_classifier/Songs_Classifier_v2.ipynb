{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Songs_Classifier_v2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "history_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hfB5cAGqxV2h"
      },
      "source": [
        "# **Preamble**\n",
        "\n",
        "**Title**: *Spotify Song Tracks Classifier*\n",
        "\n",
        "**Author**: *Dakota M. Miller*\n",
        "\n",
        "**Email**: *dmil166 @ msudenver.edu*\n",
        "\n",
        "**Last Update**: *2021-11-14*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xC8rdbmQxYcZ"
      },
      "source": [
        "# **Introduction**\n",
        "\n",
        "The goal of this analysis is to develop classification models capable of predicting the genre of a song based on measured audio attributes.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zBAUPzUkxdBk"
      },
      "source": [
        "# **Dataset**\n",
        "\n",
        "The dataset for this project is available under 'data' (genres_v2.csv) and it was obtained from [Kaggle](https://www.kaggle.com/mrmorj/dataset-of-songs-in-spotify). It consists of a collection of 42,305 song track from various musical genres, with the following audio attributes:\n",
        "\n",
        "* danceability\n",
        "* energy\n",
        "* key\n",
        "* loudness\n",
        "* speechiness\n",
        "* acousticness\n",
        "* instrumentalness\n",
        "* liveness\n",
        "* valence\n",
        "* tempo\n",
        "* duration_ms\n",
        "\n",
        "The meaning of those audio attributes are described by Spotify [here](https://developer.spotify.com/documentation/web-api/reference/#/operations/get-several-audio-features).\n",
        "\n",
        "```\n",
        "[\n",
        "danceability,energy,key,loudness,mode,speechiness,acousticness,instrumentalness,liveness,valence,tempo,type,id,uri,track_href,analysis_url,duration_ms,time_signature,genre,song_name,Unnamed: 0,title\n",
        "0.831,0.8140000000000001,2,-7.364,1,0.42,0.0598,0.0134,0.0556,0.389,156.985,audio_features,2Vc6NJ9PW9gD9q343XFRKx,spotify:track:2Vc6NJ9PW9gD9q343XFRKx,https://api.spotify.com/v1/tracks/2Vc6NJ9PW9gD9q343XFRKx,https://api.spotify.com/v1/audio-analysis/2Vc6NJ9PW9gD9q343XFRKx,124539,4,Dark Trap,Mercury: Retrograde,,\n",
        "0.7190000000000001,0.493,8,-7.23,1,0.0794,0.401,0.0,0.11800000000000001,0.124,115.08,audio_features,7pgJBLVz5VmnL7uGHmRj6p,spotify:track:7pgJBLVz5VmnL7uGHmRj6p,https://api.spotify.com/v1/tracks/7pgJBLVz5VmnL7uGHmRj6p,https://api.spotify.com/v1/audio-analysis/7pgJBLVz5VmnL7uGHmRj6p,224427,4,Dark Trap,Pathology,,\n",
        ".\n",
        ".\n",
        ".\n",
        "0.529,0.945,9,-5.862,1,0.0615,0.00189,5.45e-05,0.414,0.134,155.047,audio_features,6MAAMZImxcvYhRnxDLTufD,spotify:track:6MAAMZImxcvYhRnxDLTufD,https://api.spotify.com/v1/tracks/6MAAMZImxcvYhRnxDLTufD,https://api.spotify.com/v1/audio-analysis/6MAAMZImxcvYhRnxDLTufD,162161,4,hardstyle,,20999.0,Best of Hardstyle 2020\n",
        "]\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3WX3G6b2xhEm"
      },
      "source": [
        "# **Configuration & Data Collection**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W-iA7KI6xjYJ"
      },
      "source": [
        "## Library Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "afrJNH89xocf"
      },
      "source": [
        "import csv\n",
        "from google.colab import drive\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from numpy import ravel                                  # For matrices\n",
        "import os\n",
        "import pandas as pd\n",
        "import requests\n",
        "import seaborn as sb\n",
        "from sklearn.feature_selection import VarianceThreshold # Feature selector\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB #Import Gaussian Naive Bayes model\n",
        "from sklearn.neighbors import KNeighborsClassifier    # The k-nearest neighbor classifier\n",
        "from sklearn.pipeline import Pipeline                                  # For setting up pipeline\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "import statistics as stats\n",
        "import sys"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VPCu6k81xq-O"
      },
      "source": [
        "## Definitions/Parameters\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GyaYqAklxs_p"
      },
      "source": [
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ed3dkadFxurD"
      },
      "source": [
        "## Resources\n",
        "\n",
        "*   [Learn how to import files into Google Colab](https://towardsdatascience.com/google-colab-import-and-export-datasets-eccf801e2971)\n",
        "*   [Troubleshoot dropping dataframe columns](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.drop.html) and [again](https://stackoverflow.com/questions/38288372/unable-to-drop-a-column-from-pandas-dataframe)\n",
        "*   [Affect a column of a dataframe with a function](https://stackoverflow.com/questions/34962104/how-can-i-use-the-apply-function-for-a-single-column)\n",
        "*   [Extracting unique values and positions from NumPy Array](https://numpy.org/doc/stable/reference/generated/numpy.unique.html)\n",
        "*   [Used a mix of this site and another I can't find now to figure how to split into multiple sub-arrays based on genre](https://numpy.org/devdocs/reference/generated/numpy.unique.html)\n",
        "*   [Located resource to accurately express myself during this project](https://looks.wtf/flipping-tables)\n",
        "*   [Researched transpose function for iterative collection building](https://numpy.org/doc/stable/reference/generated/numpy.ndarray.transpose.html)\n",
        "*   [Leveraged NumPy to calculate attribute statistics, including q1 and q3](https://numpy.org/doc/stable/reference/generated/numpy.quantile.html)\n",
        "*   [Found online guide and modified approach to generate list of outliers](https://blog.finxter.com/how-to-find-outliers-in-python-easily/)\n",
        "*   Coworker suggestion to use pandas for the histogram production and seaborn library for correlation analysis\n",
        "    * [Seaborn pairplot documentation](https://seaborn.pydata.org/generated/seaborn.pairplot.html)\n",
        "*   [Refresher on classification](https://www.geeksforgeeks.org/multiclass-classification-using-scikit-learn/)\n",
        "    * [Decision Tree Classification Demonstration](https://www.datacamp.com/community/tutorials/decision-tree-classification-python)\n",
        "    * [Naive Bayes Classification Demonstration](https://www.datacamp.com/community/tutorials/naive-bayes-scikit-learn)\n",
        "    * Used In-Class Assignment 21 for the approach to Artificial Neural Network\n",
        "    * [Modeled KNN model similar to IBM example](https://developer.ibm.com/tutorials/learn-classification-algorithms-using-python-and-scikit-learn/)\n",
        "    * [Read about pipelines and how they fit/transform data in a compact manner](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html)\n",
        "    * [KNN Classifier documentation](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html)\n",
        "    * [Online illustration of KNN combined with pipelining](https://machinelearningmastery.com/modeling-pipeline-optimization-with-scikit-learn/)\n",
        "    * [Awesome explanation of KNN Classification and Pipelining](https://medium.com/analytics-vidhya/beginners-guide-to-k-nearest-neighbors-pipelines-in-classification-704b87f534e2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XdX3zqq_x0Kg"
      },
      "source": [
        "# **Preprocessing**\n",
        "\n",
        "This section should contain one or more markdown text with supporting code explaining how the dataset was processed before the analysis, including attributes removal, attributes enconding, and any transformations perfomed in the original dataset.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XC0UlI_gx4Zj"
      },
      "source": [
        "## Specific Preprocessing:\n",
        "* Remove column 'type'\n",
        "    * This column only had one value across all records and would not offer meaningful progress towards classification\n",
        "* Remove columns 'id', 'uri', 'track_href', 'analysis_url', 'song_name', 'Unnamed: 0', 'title'\n",
        "    * Many of these are either unique identifiers or offer other meaning (such as locations on the web) that should not be used for classification purposes\n",
        "* Prepared data for use in Summary Statistics as well as boxplot generation through list comprehension and iteration"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SoOLyzsAx8Lk"
      },
      "source": [
        "# Read in csv as pandas dataframe\n",
        "spotify_df = pd.read_csv('/content/drive/My Drive/genres_v2.csv')\n",
        "\n",
        "# Delete unnecessary columns\n",
        "spotify_df.drop(columns=['type', 'id', 'uri', 'track_href', 'analysis_url', 'time_signature', 'song_name', 'Unnamed: 0', 'title'], axis=1, inplace=True)\n",
        "\n",
        "# Capture column headers into a Python list\n",
        "col_headers = spotify_df.columns.to_list()\n",
        "attr_headers = col_headers[0:-1]\n",
        "\n",
        "# Capitalize genres, then sort dataframe by this column. Extract unique list of genres\n",
        "spotify_df['genre'] = spotify_df['genre'].map(lambda genre: genre.upper())\n",
        "genres_list = list(spotify_df['genre'].unique())\n",
        "genres_list.sort()\n",
        "spotify_df.sort_values('genre', inplace=True)\n",
        "\n",
        "# Convert data to NumPy Array and confirm genre sorting is still in place\n",
        "spotify_nparray = spotify_df.to_numpy()\n",
        "\n",
        "# Capture unique genres (mainly for indices)\n",
        "genres_nparray, indices = np.unique(spotify_nparray[:, -1], return_index=True)\n",
        "\n",
        "# Split large NumPy Array into sub-arrays by genre\n",
        "genres_nparray_list = np.array_split(spotify_nparray, indices)\n",
        "all_genres_nparrays_list = []\n",
        "for x in genres_nparray_list[1:]:\n",
        "  all_genres_nparrays_list.append(np.array(x))\n",
        "\n",
        "# Pivot genre sub-array for each attribute,\n",
        "# Append results to list of lists of values (attribute lists of genre lists of metric data)\n",
        "genre_attr_index = 0\n",
        "all_attribute_genre_data = [[ [] for _ in range(len(genres_list)) ] for _ in range (len(attr_headers)) ]\n",
        "# histogram_attribute_lists2 = [[] for _ in all_attribute_genre_data] #approach to simplify aggregations into one pass over the data\n",
        "\n",
        "# Generates transposed data for purposes of boxplot iteration\n",
        "for a in all_genres_nparrays_list:\n",
        "  attr_iter_index = 0 \n",
        "  metrics = all_genres_nparrays_list[genre_attr_index]\n",
        "  for row in metrics.transpose()[:-1]:\n",
        "    all_attribute_genre_data[attr_iter_index][genre_attr_index] = list(row)\n",
        "    # histogram_attribute_lists2[attr_iter_index].append(list(row))\n",
        "    attr_iter_index += 1\n",
        "  genre_attr_index += 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rmOwBKZSx9yM"
      },
      "source": [
        "# **Summary Statistics, Boxplots, and Histograms**\n",
        "\n",
        "This section should present the python code that displays summary statistics and visuals from each of the attributes of the dataset; at minimum, a histogram and a multi box plot (per genre) should be produce per attribute.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nKekzgoWx_j_"
      },
      "source": [
        "## Summary Stats\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yAK1crTNyDo2"
      },
      "source": [
        "# Establish indices for iterating in summary statistic production\n",
        "col_index = 0\n",
        "genre_index = 0\n",
        "\n",
        "# Iterate through each genre set of attributes\n",
        "while genre_index < len(all_genres_nparrays_list):\n",
        "  print('\\n\\nSUMMARY STATISTICS FOR', genres_list[genre_index], '\\n')\n",
        "  current_genre_nparray = all_genres_nparrays_list[genre_index]\n",
        "  genre_outliers_tuple = []\n",
        "  \n",
        "  # For each attribute, calculate and output summary statistics\n",
        "  while col_index < (len(col_headers) - 1): #remove 1 for genres\n",
        "    temp_array = current_genre_nparray[:, col_index]\n",
        "    current_attribute = col_headers[col_index].upper()\n",
        "    print('*** Summary Statistics of Attribute: ' + current_attribute + ' ***')\n",
        "    attribute_min = '{:,.3f}'.format(np.min(temp_array))\n",
        "    attribute_max = '{:,.3f}'.format(np.max(temp_array))\n",
        "    print(f'{current_attribute} Range: [{attribute_min} , {attribute_max}]')\n",
        "    attribute_mean = '{:,.3f}'.format(np.mean(temp_array))\n",
        "    print(f'{current_attribute} Mean: {attribute_mean}')\n",
        "    attribute_median = '{:,.3f}'.format(np.median(temp_array))\n",
        "    print(f'{current_attribute} Median: {attribute_median}')\n",
        "    attribute_std = '{:,.3f}'.format(np.std(temp_array))\n",
        "    print(f'{current_attribute} StD: {attribute_std}')\n",
        "    col_index += 1\n",
        "  genre_index += 1\n",
        "  col_index = 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WKbVWPbLd2Sp"
      },
      "source": [
        "## Boxplots"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6TtfwCfRdxg8"
      },
      "source": [
        "# Iterate through prep'd attribute-genre 2d array of metrics and data for boxplots\n",
        "attr_str_index = 0\n",
        "for attribute_metrics in all_attribute_genre_data:\n",
        "  data = all_attribute_genre_data[attr_str_index]\n",
        "  print('data:', data)\n",
        "  medians = [ stats.median(branch) for branch in data ]\n",
        "  plt.boxplot(\n",
        "      data, vert=False\n",
        "  )\n",
        "  i = 0\n",
        "  branch_labels = []\n",
        "  for branch in data:\n",
        "    max_value = max(branch)\n",
        "    branch_labels.append(genres_list[i])\n",
        "    i += 1\n",
        "  axes = plt.gca()\n",
        "  axes.spines['right'].set_visible(False)\n",
        "  axes.spines['top'].set_visible(False)\n",
        "  axes.set_yticklabels(branch_labels)\n",
        "  plt.xlabel(attr_headers[attr_str_index])\n",
        "  plt.show()\n",
        "  attr_str_index += 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N2S8ZkuJeA7H"
      },
      "source": [
        "## Histograms"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ezfe17CmeDIQ"
      },
      "source": [
        "for attr in attr_headers:\n",
        "  plt.figure()\n",
        "  spotify_df[attr].hist()\n",
        "  plt.title(attr)\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ciClPgoSyFeq"
      },
      "source": [
        "# **Correlation Analysis**\n",
        "\n",
        "Do at least one correlation analysis, like comparing speechiness and instrumentalness, for example.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T5ineHjEyJcF"
      },
      "source": [
        "# Correlation analysis on last half of attributes\n",
        "sb.pairplot(spotify_df.iloc[:, 6:], corner=True, diag_kind=None, hue='genre')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SgqSEIclyRBd"
      },
      "source": [
        "# **Classification**\n",
        "\n",
        "Show the code and results of 3 classification models based on decision tree inference, naive Bayes classification, and artificial neural networks; each classification model should be based on a consistent split between training and testing datasets; the accuracy of each model should be displayed; any specific tunning parameter used in a model should be supported by an explanatory text.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "slPhdYEGySMK"
      },
      "source": [
        "## Decision Tree\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LFBLfvddyTf3"
      },
      "source": [
        "from sklearn import metrics #Import scikit-learn metrics module for accuracy calculation\n",
        "\n",
        "dt_columns = spotify_df.columns\n",
        "feature_columns = dt_columns[0:-1]\n",
        "X = spotify_df[feature_columns]\n",
        "y = spotify_df.genre\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1) # 70% training and 30% test\n",
        "\n",
        "# Create Decision Tree classifer object\n",
        "clf = DecisionTreeClassifier()\n",
        "\n",
        "# Train Decision Tree Classifer\n",
        "clf = clf.fit(X_train,y_train)\n",
        "\n",
        "#Predict the response for test dataset\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "# Model Accuracy, how often is the classifier correct?\n",
        "decision_tree_result = \"Decision Tree Accuracy: \" + str(metrics.accuracy_score(y_test, y_pred))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qN1R_gdjtepW"
      },
      "source": [
        "## Naive Bayes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bDgrwpDgtgZX"
      },
      "source": [
        "gnb = GaussianNB() #Create a Gaussian Classifier\n",
        "\n",
        "gnb.fit(X_train, y_train) #Train the model using the training sets\n",
        "\n",
        "y_pred = gnb.predict(X_test) #Predict the response for test dataset\n",
        "\n",
        "naive_bayes_result = \"\\nNaive Bayes Accuracy: \" + str(metrics.accuracy_score(y_test, y_pred)) # Model Accuracy, how often is the classifier correct?"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JllQBwduthRm"
      },
      "source": [
        "## Artifical Neural Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AQSTYufztkeQ"
      },
      "source": [
        "# Current parameters for ANN take between a few-several minutes to run. \n",
        "from sklearn.neural_network import MLPClassifier \n",
        "from sklearn import preprocessing  \n",
        "  \n",
        "# Split the dataset into training and test dataset (assume the training set is 20% of the whole dataset)\n",
        "df_train, df_test = train_test_split(spotify_df, test_size=0.3, random_state=0)\n",
        "\n",
        "# Normalize attribute values first  \n",
        "min_max_scaler = preprocessing.MinMaxScaler()\n",
        "\n",
        "# MinMaxScaler used to transform data to fall between 0 and 1\n",
        "X_train = min_max_scaler.fit_transform(df_train.iloc[:,:-1].values)\n",
        "X_test = min_max_scaler.fit_transform(df_test.iloc[:,:-1].values)\n",
        "Y_train = df_train.iloc[:,-1].values\n",
        "Y_test = df_test.iloc[:,-1].values\n",
        "\n",
        "# Train a multilayer ANN to implement a hearts (disease) classifier\n",
        "clf = MLPClassifier(max_iter=3000, random_state=0)\n",
        "clf.fit(X_train, Y_train)\n",
        "\n",
        "# Accuracy of the classifier\n",
        "ann_results = '\\nArtificial Neural Network Accuracy: ' + str(clf.score(X_test, Y_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vVNK5CaSAch9"
      },
      "source": [
        "## K-Nearest Neighbors"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bHu_rvCwDanX"
      },
      "source": [
        "# Overwrite existing assignments for train/test splits but use same split as original from Decision Tree\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1) # 70% training and 30% test\n",
        "\n",
        "# Illustrate KNN performance without preprocessing data\n",
        "knn_before_results = \"\\nKNN performance before pipelining dataset:\"\n",
        "knn = KNeighborsClassifier().fit(X_train, y_train)\n",
        "knn_before_results += '\\n\\tAccuracy: ' + str(knn.score(X_test,y_test))\n",
        "\n",
        "# Create a pipeline to scale and process the data prior to model usage\n",
        "pipe = Pipeline([\n",
        "('scaler', StandardScaler()),\n",
        "('selector', VarianceThreshold()),\n",
        "('classifier', KNeighborsClassifier())\n",
        "])\n",
        "\n",
        "# Apply pipeline with specific conditions to training data\n",
        "pipe.fit(X_train, y_train)\n",
        "\n",
        "# Illustrate KNN performance after preprocessing (fitting/transforming) data\n",
        "knn_after_results = '\\nKNN performance after pipelining dataset:'\n",
        "knn_after_results += '\\n\\tAccuracy: ' + str(pipe.score(X_test,y_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "251QZYt8VrtW"
      },
      "source": [
        "### Output all results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JmE0hNRgVrAG"
      },
      "source": [
        "# Output all results side-by-side for easy viewing\n",
        "print(decision_tree_result, naive_bayes_result, ann_results, knn_before_results, knn_after_results)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}