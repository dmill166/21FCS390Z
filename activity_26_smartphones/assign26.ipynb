{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Preamble\n",
    "\n",
    "Title: *Smartphones Cluster Analysis*\n",
    "\n",
    "Author: *Dakota M. Miller*\n",
    "\n",
    "Email: *dmiller @ msudenver.edu*\n",
    "\n",
    "Last Update: *2021-11-16*"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Introduction\n",
    "\n",
    "The goal of this activity is to perform a data analysis aiming to group together smartphones that have similar specs.\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Dataset\n",
    "\n",
    "The dataset for this report was created from information available [https://www.productchart.com/smartphones/](https://www.productchart.com/smartphones/). For each smartphone, the referred website maintains the following specs (among others):\n",
    "\n",
    "* screen size (in inches)\n",
    "* screen resolution (in pixels x pixels)\n",
    "* storage (in GB)\n",
    "* ram (in GB)\n",
    "* clock speed (in GHz)\n",
    "* number of cores\n",
    "* weight (in ounces)\n",
    "* rear camera resolution (in MP)\n",
    "* front camera resolution (in MP)\n",
    "* battery (in mah)\n",
    "* price (in US$)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Summary Statistics\n",
    "\n",
    "## Headlines per Year"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Google drive mount\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# CS390Z - Introduction to Data Mining - Fall 2021\n",
    "# Instructor: Thyago Mota\n",
    "# Description: Headlines per Year\n",
    "\n",
    "from google.colab import drive\n",
    "import csv\n",
    "import statistics as stats\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# definitions/parameters\n",
    "DATA_FOLDER = '/content/drive/MyDrive/Colab Datasets/abc_news_dataset/'\n",
    "CSV_FILE_NAME = 'abcnews-date-text.csv'\n",
    "\n",
    "with open(DATA_FOLDER + CSV_FILE_NAME, 'rt') as csv_file:\n",
    "  reader = csv.reader(csv_file)\n",
    "  header = next(reader)\n",
    "  years_count = {}\n",
    "  for row in reader:\n",
    "    year = int(row[0][:4])\n",
    "    if year not in years_count:\n",
    "      years_count[year] = 0\n",
    "    years_count[year] += 1\n",
    "  print('Number of years in dataset:', len(years_count))\n",
    "  print('Headlines per year:')\n",
    "  for year in years_count:\n",
    "    print(year,years_count[year])\n",
    "  print('The avg. number of headlines per year was', '{:.2f}'.format(stats.mean(years_count.values())))\n",
    "  print('The std. of the number of headlines per year was', '{:.2f}'.format(stats.stdev(years_count.values())))\n",
    "  years = [ str(year)[2:] for year in years_count.keys()]\n",
    "  plt.bar(years, height=years_count.values())\n",
    "  plt.xticks(years)\n",
    "  plt.xlabel('Year')\n",
    "  plt.ylabel('Number of Headlines')\n",
    "  plt.show()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# NLP Analysis\n",
    "\n",
    "## News Headline Corpora / Year\n",
    "\n",
    "To speed-up the analysis a 5% sampling was perfomed when computing the corpora of news headlines.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# CS390Z - Introduction to Data Minining - Fall 2021\n",
    "# Instructor: Thyago Mota\n",
    "# Description: NLP (news headline corpora per year)\n",
    "\n",
    "import csv, random\n",
    "\n",
    "# definitions/parameters\n",
    "DATA_FOLDER = '/content/drive/MyDrive/Colab Datasets/abc_news_dataset/'\n",
    "CSV_FILE_NAME = 'abcnews-date-text.csv'\n",
    "SAMPLE_RATE = 5 # like in 5%\n",
    "\n",
    "with open(DATA_FOLDER + CSV_FILE_NAME, 'rt') as csv_file:\n",
    "  reader = csv.reader(csv_file)\n",
    "  header = next(reader)\n",
    "  headline_corpora = [ '' for i in range(18) ]\n",
    "  for row in reader:\n",
    "    if random.randint(1, 100) <= SAMPLE_RATE:\n",
    "      year = int(row[0][:4])\n",
    "      headline = row[1]\n",
    "      headline_corpora[year - 2003] += headline + \". \""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Top Words / Year"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# NLTK package download\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('vader_lexicon')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# CS390Z - Introduction to Data Minining - Fall 2021\n",
    "# Instructor: Thyago Mota\n",
    "# Description: NLP (top words per year)\n",
    "\n",
    "import string, operator\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "YEAR = 2020\n",
    "\n",
    "stop = set(stopwords.words('english') + list(string.punctuation))\n",
    "word_tokens = word_tokenize(headline_corpora[YEAR - 2003])\n",
    "word_tokens = [ word for word in word_tokens if word not in stop ]\n",
    "\n",
    "count_vect = CountVectorizer()\n",
    "bag_of_words = count_vect.fit_transform(word_tokens).toarray()\n",
    "words = count_vect.get_feature_names()\n",
    "words_freq = {}\n",
    "for row in bag_of_words:\n",
    "  for i in range(len(row)):\n",
    "    word = words[i]\n",
    "    if word not in words_freq:\n",
    "      words_freq[word] = 0\n",
    "    words_freq[word] += row[i]\n",
    "words_freq = sorted(words_freq.items(), key=lambda item: item[1], reverse=True)\n",
    "print(\"Top 10 words found in news headlines in\", YEAR)\n",
    "for i in range(10):\n",
    "  print(words_freq[i])\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Top Bigrams / Year"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# CS390Z - Introduction to Data Minining - Fall 2021\n",
    "# Instructor: Thyago Mota\n",
    "# Description: NLP (top bigrams per year)\n",
    "\n",
    "import string, operator\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "YEAR = 2017\n",
    "\n",
    "sent_tokens = sent_tokenize(headline_corpora[YEAR - 2003])\n",
    "count_vect = CountVectorizer(stop_words=\"english\", ngram_range=(2,2))\n",
    "bag_of_bigrams = count_vect.fit_transform(sent_tokens).toarray()\n",
    "bigrams = count_vect.get_feature_names()\n",
    "bigrams_freq = {}\n",
    "for row in bag_of_bigrams:\n",
    "  for i in range(len(row)):\n",
    "    bigram = bigrams[i]\n",
    "    if bigram not in bigrams_freq:\n",
    "      bigrams_freq[bigram] = 0\n",
    "    bigrams_freq[bigram] += row[i]\n",
    "bigrams_freq = sorted(bigrams_freq.items(), key=lambda item: item[1], reverse=True)\n",
    "print(\"Top 10 bigrams found in news headlines in\", YEAR)\n",
    "for i in range(10):\n",
    "  print(bigrams_freq[i])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Top Topics / Year"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# CS390Z - Introduction to Data Minining - Fall 2021\n",
    "# Instructor: Thyago Mota\n",
    "# Description: NLP (top topics per year)\n",
    "\n",
    "import string, operator\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "YEAR = 2011\n",
    "\n",
    "# returns the indices of the top n values of a numpy array\n",
    "def top_indices(data, n):\n",
    "  return data.argsort()[-1*n:][::-1]\n",
    "\n",
    "sent_tokens = sent_tokenize(headline_corpora[YEAR - 2003])\n",
    "count_vect = CountVectorizer(stop_words=\"english\", ngram_range=(2,2))\n",
    "bag_of_bigrams = count_vect.fit_transform(sent_tokens).toarray()\n",
    "bigrams = count_vect.get_feature_names()\n",
    "lda = LatentDirichletAllocation(n_components=5, random_state=0)\n",
    "lda.fit(bag_of_bigrams)\n",
    "print(\"Top topics found in news headlines in\", YEAR)\n",
    "for i, topic in enumerate(lda.components_):\n",
    "  print (\"Topic \", i, \", \".join(bigrams[j] for j in top_indices(topic, 5)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Sentiment Fluctuation over the Years"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# CS390Z - Introduction to Data Minining - Fall 2021\n",
    "# Instructor: Thyago Mota\n",
    "# Description: NLP (sentiment fluctuation over the years)\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "import matplotlib.pyplot as plt\n",
    "import statistics as stats\n",
    "\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "sent_year = [ [] for i in range(18) ]\n",
    "for year in range(2003, 2021):\n",
    "  corpus = headline_corpora[year - 2003]\n",
    "  sent_tokens = sent_tokenize(headline_corpora[year - 2003])\n",
    "  for sent in sent_tokens:\n",
    "    score = sia.polarity_scores(sent)[\"compound\"]\n",
    "    sent_year[year - 2003].append(score)\n",
    "plt.boxplot(sent_year, vert=False)\n",
    "plt.yticks(list(range(1, 19)), list(range(2003,2021)))\n",
    "plt.xlabel('Sentiment Score')\n",
    "plt.ylabel('Year')\n",
    "plt.show()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}